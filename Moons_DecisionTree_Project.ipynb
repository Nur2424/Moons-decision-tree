{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fcdc688",
   "metadata": {},
   "source": [
    "# Decision Tree Classification on the Moons Dataset\n",
    "\n",
    "A small, end-to-end **machine learning workflow** (inspired by *Hands-On Machine Learning*, Chapter 6):\n",
    "\n",
    "1. Generate a noisy, non-linear dataset (`make_moons`)\n",
    "2. Split into train/test\n",
    "3. Train a baseline Decision Tree\n",
    "4. Tune hyperparameters with cross-validation (`GridSearchCV`)\n",
    "5. Evaluate on the test set\n",
    "6. Visualize decision boundaries\n",
    "\n",
    "> **Goal:** learn the workflow and build intuition about **overfitting** and **regularization** for Decision Trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7d121",
   "metadata": {},
   "source": [
    "## 1) Imports & Reproducibility\n",
    "\n",
    "We fix a `random_state` so results are repeatable (important for debugging and for GitHub projects).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bb07a",
   "metadata": {},
   "source": [
    "## 2) Data Generation & Quick Exploration\n",
    "\n",
    "`make_moons` returns:\n",
    "- `X`: features with shape `(n_samples, 2)`\n",
    "- `y`: labels (0 or 1)\n",
    "\n",
    "The `noise` parameter makes the task harder by adding overlap between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309367bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=10_000, noise=0.4, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154849c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
    "plt.title(\"Moons dataset (n_samples=10,000, noise=0.4)\")\n",
    "plt.xlabel(\"Feature 1 (x1)\")\n",
    "plt.ylabel(\"Feature 2 (x2)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb561b8",
   "metadata": {},
   "source": [
    "## 3) Trainâ€“Test Split\n",
    "\n",
    "We keep the **test set** untouched until the very end to measure generalization.\n",
    "We also use `stratify=y` to preserve class proportions in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe68acb",
   "metadata": {},
   "source": [
    "## 4) Baseline Model (Default Decision Tree)\n",
    "\n",
    "A default Decision Tree often **overfits** noisy data:\n",
    "- training accuracy becomes very high\n",
    "- test accuracy is lower\n",
    "\n",
    "This baseline gives us a reference point before tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff137e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "baseline_tree.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = baseline_tree.predict(X_train)\n",
    "y_test_pred = baseline_tree.predict(X_test)\n",
    "\n",
    "baseline_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "baseline_test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Baseline training accuracy: {baseline_train_acc:.4f}\")\n",
    "print(f\"Baseline test accuracy:     {baseline_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c89de",
   "metadata": {},
   "source": [
    "## 5) Hyperparameter Tuning with Cross-Validation\n",
    "\n",
    "We tune `max_leaf_nodes` (as suggested in the book).  \n",
    "This limits the number of leaves and acts as **regularization** (prevents the tree from growing too complex).\n",
    "\n",
    "We use 5-fold cross-validation on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_leaf_nodes\": list(range(2, 101))  # try 2..100\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bcbde",
   "metadata": {},
   "source": [
    "## 6) Final Model (Train on full training set, evaluate on test set)\n",
    "\n",
    "After selecting the best hyperparameters via CV, we evaluate once on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec88a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = grid_search.best_estimator_\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_best = best_tree.predict(X_test)\n",
    "best_test_acc = accuracy_score(y_test, y_test_pred_best)\n",
    "\n",
    "print(f\"Tuned test accuracy: {best_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, y_test_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_best)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e47608",
   "metadata": {},
   "source": [
    "## 7) Visualizing Decision Boundaries\n",
    "\n",
    "Decision Trees create **axis-aligned** splits, so boundaries often look like rectangles/steps.\n",
    "\n",
    "We'll plot boundaries for both:\n",
    "- baseline tree (likely overfit)\n",
    "- tuned tree (simpler, better generalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbebdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, title=\"Decision boundary\"):\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.linspace(x1_min, x1_max, 400),\n",
    "        np.linspace(x2_min, x2_max, 400)\n",
    "    )\n",
    "    X_grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    preds = clf.predict(X_grid).reshape(xx1.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx1, xx2, preds, alpha=0.25)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1 (x1)\")\n",
    "    plt.ylabel(\"Feature 2 (x2)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ebc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(\n",
    "    baseline_tree, X_train, y_train,\n",
    "    title=f\"Baseline tree decision boundary (train) | acc={baseline_train_acc:.3f}\"\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "    best_tree, X_train, y_train,\n",
    "    title=f\"Tuned tree decision boundary (train) | max_leaf_nodes={grid_search.best_params_['max_leaf_nodes']}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19407166",
   "metadata": {},
   "source": [
    "## 8) Summary\n",
    "\n",
    "- Baseline Decision Tree often **overfits** noisy data (high train accuracy, lower test accuracy).\n",
    "- Limiting complexity with `max_leaf_nodes` improves **generalization**.\n",
    "- Cross-validation helps choose hyperparameters without peeking at the test set.\n",
    "\n",
    "### Next steps\n",
    "- Try additional regularization: `max_depth`, `min_samples_leaf`\n",
    "- Compare with **RandomForestClassifier** (ensembles reduce instability / variance)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
